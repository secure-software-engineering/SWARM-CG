runner_config:
  max_new_tokens: 1024
  temperature: 0.001

models:
# - name: "qwen2-it-7b"
#   model_path: "Qwen/Qwen2-7B-Instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12
#   comment: "Does not support VLLMs with quantization bitsandbytes"

# - name: "qwen2-it-72b"
#   model_path: "Qwen/Qwen2-72B-Instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12
#   comment: "Does not support VLLMs with quantization bitsandbytes"

# - name: "gemma2-it-9b"
#   model_path: "google/gemma-2-9b-it"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: false
#   use_vllms_for_evaluation: false
#   max_model_len: 4096
#   batch_size: 12
#   torch_dtype: "bfloat16"

# - name: "gemma2-it-27b"
#   model_path: "google/gemma-2-27b-it"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: false
#   use_vllms_for_evaluation: false
#   max_model_len: 4096
#   batch_size: 12
#   torch_dtype: "bfloat16"

# - name: "gemma2-it-2b"
#   model_path: "google/gemma-2-2b-it"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: false
#   use_vllms_for_evaluation: false
#   max_model_len: 4096
#   batch_size: 12
#   torch_dtype: "bfloat16"

# - name: "codellama-it-7b"
#   model_path: "meta-llama/CodeLlama-7b-Instruct-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "codellama-it-13b"
#   model_path: "meta-llama/CodeLlama-13b-Instruct-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "codellama-it-34b"
#   model_path: "meta-llama/CodeLlama-34b-Instruct-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "llama3.1-it-8b"
#   model_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "llama3.1-it-70b"
#   model_path: "meta-llama/Meta-Llama-3.1-70B-Instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

- name: "tinyllama-1.1b"
  model_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  quantization: "bitsandbytes"
  lora_repo: null
  use_system_prompt: true
  use_vllms_for_evaluation: false
  max_model_len: 2048
  batch_size: 12

# - name: "phi3-small-it-7.3b"
#   model_path: "microsoft/Phi-3-small-128k-instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "phi3-medium-it-14b"
#   model_path: "microsoft/Phi-3-medium-128k-instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "phi3-mini-it-3.8b"
#   model_path: "microsoft/Phi-3-mini-128k-instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "phi3.5-mini-it-3.8b"
#   model_path: "microsoft/Phi-3.5-mini-instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "phi3.5-moe-it-41.9b"
#   model_path: "microsoft/Phi-3.5-MoE-instruct"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "mixtral-v0.1-it-8x22b"
#   model_path: "mistralai/Mixtral-8x22B-Instruct-v0.1"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12
#   comment: "Does not work with 1 GPU"

# - name: "mixtral-v0.1-it-8x7b"
#   model_path: "mistralai/Mixtral-8x7B-Instruct-v0.1"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "mistral-v0.3-it-7b"
#   model_path: "mistralai/Mistral-7B-Instruct-v0.3"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "mistral-nemo-it-2407-12.2b"
#   model_path: "mistralai/Mistral-Nemo-Instruct-2407"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "mistral-large-it-2407-123b"
#   model_path: "mistralai/Mistral-Large-Instruct-2407"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "codestral-v0.1-22b"
#   model_path: "mistralai/Codestral-22B-v0.1"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# MODELS NOT SUPPORTED CURRENTLY! 

# - name: "codellama-python-7b"
#   model_path: "meta-llama/CodeLlama-7b-Python-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "codellama-python-13b"
#   model_path: "meta-llama/CodeLlama-13b-Python-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

# - name: "codellama-python-34b"
#   model_path: "meta-llama/CodeLlama-34b-Python-hf"
#   quantization: "bitsandbytes"
#   lora_repo: null
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: 8192
#   batch_size: 12

custom_models:
  # - name: "tinyllama-ft-1.1b"
  #   model_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  #   quantization: "bitsandbytes"
  #   lora_repo: "/home/ssegpu/fine-tuning-apsv/models_single_label/TinyLlama-TinyLlama-1-1B-Chat-v1-0-single-label-v1"
  #   use_system_prompt: true
  #   use_vllms_for_evaluation: true
  #   max_model_len: 2048
  #   batch_size: 12


openai_models:
# - name: "gpt-3.5-turbo"
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: null

# - name: "gpt-4o"
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: null

# - name: "gpt-4o-mini"
#   use_system_prompt: true
#   use_vllms_for_evaluation: false
#   max_model_len: null
